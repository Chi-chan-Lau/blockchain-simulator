{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "proposal_rates = np.linspace(0.001, 0.1, 1)\n",
    "n_trials = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create experimental parameters and write to JSON file\n",
    "\n",
    "params = {'Block proposal rate parameter': 0,\n",
    " 'Block size (txs)': 50,\n",
    " 'Duration (sec)': 0,\n",
    " 'Fork choice rule': 'GHOST',\n",
    " 'Network model': 'Decker-Wattenhorf',\n",
    " 'Number of adversaries': 1,\n",
    " 'Number of nodes': 100,\n",
    " 'Probability of error in transaction confirmation': 0.1,\n",
    " 'Transaction dataset': 'poisson'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{'GHOST-test': {'Block proposal rate parameter': 0.001,\n",
      "                'Block size (txs)': 50,\n",
      "                'Duration (sec)': 2400,\n",
      "                'Fork choice rule': 'GHOST',\n",
      "                'Network model': 'Decker-Wattenhorf',\n",
      "                'Number of adversaries': 1,\n",
      "                'Number of nodes': 100,\n",
      "                'Probability of error in transaction confirmation': 0.1,\n",
      "                'Transaction dataset': 'poisson'},\n",
      " 'setting-name': 'GHOST-test'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/transactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c32a1ea8c93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 main.py -f results/GHOST.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mthroughput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_throughput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mavg_main_chain_arrival_latency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_finalization_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_latency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mthroughputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthroughput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmain_chain_arrival_latencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_main_chain_arrival_latency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/blockchain-simulator/metrics.py\u001b[0m in \u001b[0;36mcompute_latency\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_latency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/transactions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmain_chain_arrival_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/transactions.csv'"
     ]
    }
   ],
   "source": [
    "import os, shutil, pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "os.system('touch GHOST.json')\n",
    "os.chdir('..')\n",
    "\n",
    "import metrics\n",
    "\n",
    "throughputs = {}\n",
    "main_chain_arrival_latencies = {}\n",
    "optimistic_confirmation_times = {}\n",
    "finalization_latencies = {}\n",
    "\n",
    "for i in range(0, len(proposal_rates)):\n",
    "    rate = proposal_rates[i]\n",
    "    params['Block proposal rate parameter'] = rate\n",
    "    params['Duration (sec)'] = 2400\n",
    "    d = {}\n",
    "    d['setting-name'] = f'GHOST-test'\n",
    "    d[f'GHOST-test'] = params\n",
    "    print('Parameters:')\n",
    "    pp.pprint(d)\n",
    "    with open('results/GHOST.json', 'w+') as outfile:\n",
    "        json.dump(d, outfile)\n",
    "    throughputs[rate] = []\n",
    "    main_chain_arrival_latencies[rate] = []\n",
    "    finalization_latencies[rate] = []\n",
    "    optimistic_confirmation_times[rate] = []\n",
    "    for trial in range(0, n_trials):\n",
    "        os.system('python3 main.py -f results/GHOST.json')\n",
    "        throughput = metrics.compute_throughput()\n",
    "        optimistic_confirmation_time = metrics.compute_optimistic_confirmation_time()\n",
    "        avg_main_chain_arrival_latency, avg_finalization_latency = metrics.compute_latency()\n",
    "        throughputs[rate].append(throughput)\n",
    "        main_chain_arrival_latencies[rate].append(avg_main_chain_arrival_latency)\n",
    "        finalization_latencies[rate].append(avg_finalization_latency)\n",
    "        optimistic_confirmation_times[rate].append(optimistic_confirmation_time)\n",
    "\n",
    "\n",
    "os.chdir('results/')\n",
    "os.remove('GHOST.json')\n",
    "\n",
    "print('Throughputs:\\n', throughputs)\n",
    "print('Main Chain Arrival Latencies:\\n', main_chain_arrival_latencies)\n",
    "print('Finalization Latencies:\\n', finalization_latencies)\n",
    "print('Optimistic Confirmation Times:\\n', optimistic_confirmation_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Averages\n",
    "\n",
    "Average main chain latency is the time for a transaction to be added to the global blocktree - the timestamp the transaction was created.\n",
    "\n",
    "Average finalization latency is the time for a transaction to be k blocks deep - to be added to the global blocktree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_main_chain_latencies = {}\n",
    "for rate in main_chain_arrival_latencies:\n",
    "    avg_main_chain_latencies[rate] = sum(main_chain_arrival_latencies[rate])/len(main_chain_arrival_latencies[rate])\n",
    "\n",
    "print(\"Average Main Chain Arrival Latency:\", avg_main_chain_latencies)\n",
    "avg_finalization_latencies = {}\n",
    "for rate in finalization_latencies:\n",
    "    avg_finalization_latencies[rate] = sum(finalization_latencies[rate])/len(finalization_latencies[rate])\n",
    "    \n",
    "print(\"Average Finalization Latency:\", avg_finalization_latencies)\n",
    "\n",
    "avg_optimistic_confirmation_times = {}\n",
    "for rate in optimistic_confirmation_times:\n",
    "    avg_optimistic_confirmation_times[rate] = sum(optimistic_confirmation_times[rate])/len(optimistic_confirmation_times[rate])\n",
    "    \n",
    "print(\"Average Optimistic Confirmation Time:\", avg_optimistic_confirmation_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Finalization Depth and Delta for Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../logs/stats.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if row[0]=='Finalization depth':\n",
    "            k = int(row[1])\n",
    "        elif row[0]=='Average network latency for blocks (sec)':\n",
    "            delta = float(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "x = avg_main_chain_latencies.keys()\n",
    "y1 = avg_main_chain_latencies.values()\n",
    "expected_y1 = (1.0/delta+1.0/proposal_rates)\n",
    "y2 = avg_finalization_latencies.values()\n",
    "expected_y2 = k*(delta+1.0/proposal_rates)\n",
    "y3 = avg_optimistic_confirmation_times.values()\n",
    "expected_y3 = 1.0/proposal_rates\n",
    "\n",
    "plt.xlabel('Block Proposal Rate (blocks/sec)')\n",
    "plt.ylabel('Main Chain Arrival Latency (sec)')\n",
    "plt.title(f'Main Chain Arrival Latency vs. Proposal Rate (k={k}, delta={delta})')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(x, y1, label='observed rate')\n",
    "plt.plot(x, expected_y1, '--', label='expected rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Block Proposal Rate (blocks/sec)')\n",
    "plt.ylabel('Finalization Latency (sec)')\n",
    "plt.title(f'Finalization Latency vs. Proposal Rate (k={k}, delta={delta})')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(x, y2, label='observed rate')\n",
    "plt.plot(x, expected_y2, '--', label='expected rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Block Proposal Rate (blocks/sec)')\n",
    "plt.ylabel('Optimistic Confirmation Time (sec)')\n",
    "plt.title(f'Optimistic Confirmation Time vs. Proposal Rate (k={k}, delta={delta})')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(x, y3, label='observed rate')\n",
    "plt.plot(x, expected_y3, '--', label='expected rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
